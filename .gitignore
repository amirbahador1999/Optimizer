import numpy as np
import matplotlib.pyplot as plt
def func(x1,x2):
  return x1**2+2*x2**2
def func_grad(x1,x2):
  return(2*x1,4*x2)   
class Optimizers:
  def __init__(self,lr_schedule,rate=0.01):
    self.lr_schedule=self.lr(lr_schedule,rate)
  def lr(self,lr_schedule,rate):
    if  lr_schedule=="constant":
      return  self.constant_lr(rate)
  def constant_lr(self,rate):
    return rate
  def gradient_descent(self,x1,x2,t1,t2,func_grad):
    gr_x1,gr_x2=func_grad(x1,x2)
    return x1-self.lr_schedule*gr_x1 , x2-self.lr_schedule*gr_x2 ,0,0
def search(optimizer,func_grad,iteration=20):
  x1,x2,t1,t2=-5,-2,0,0
  x_iterations=[(x1,x2)]
  for i in range(iteration):
    x1,x2,t1,t2=optimizer(x1,x2,t1,t2,func_grad)
    x_iterations.append((x1,x2))
  print("epoch{},x1:{},x2:{}".format(i+1,x1,x2))
  return x_iterations    
def search_path(func,x_iterations):
  plt.plot(*zip(*(x_iterations)),"-o",color="r")
  x1,x2=np.meshgrid(np.arange(-5.5,5.5,0.1),np.arange(-5.5,5.5,0.1))
  plt.contourf(x1,x2,func(x1,x2))
  plt.xlabel("x1")
  plt.ylabel("x2") 
opt=Optimizers("constant",rate=0.1)
search_path(func,search(opt.gradient_descent,func_grad))  
